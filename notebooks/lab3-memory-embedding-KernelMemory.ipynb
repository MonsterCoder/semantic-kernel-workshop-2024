{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab3-KernelMemory-embedding-RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have seen a lots of applications like \"Chat to your own data\" or so on. This is the most common LLM architecture parent name RAG . A simple RAG flow will be like\n",
    "\n",
    "1 Retrieval:\n",
    "retrievs relevant information from a pre-existing database, knowledge graph, or corpus. This retrieved information serves as contextual knowledge for the generative model.\n",
    "The retrieval step involves querying the database using techniques like keyword search, semantic similarity, or more advanced methods like dense retrieval.\n",
    "\n",
    "2 Augmentation:\n",
    "\n",
    "The retrieved information is then integrated or augmented into the generative model.\n",
    "This augmentation process enhances the generative model's understanding of the context by providing relevant background information.\n",
    "\n",
    "3 Generation:\n",
    "\n",
    "With the augmented context, the generative model produces output.\n",
    "This output could be in the form of text generation, such as answering a question, completing a sentence, or generating a full document.\n",
    "\n",
    "4 Refinement (optional):\n",
    "Optionally, the generated output can undergo refinement or post-processing steps to ensure coherence, correctness, and fluency.\n",
    "Refinement techniques may include language model fine-tuning, paraphrasing, or other forms of text improvement.\n",
    "\n",
    "5 Output:\n",
    "\n",
    "The final output is delivered to the user or downstream application.\n",
    "This output benefits from both the generative capabilities of the model and the contextual knowledge retrieved during the process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a simple RAG using KernelMemory - the easy path \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "The initial step involves loading our data into a vector store.\n",
    "\n",
    "Textual information is encoded as long vectors of numbers, termed \"embeddings.\" The semantic similarity of stored text is gauged by the distance between two vectors in a high-dimensional space. Upon querying, the input is transformed into an embedding vector and contrasted against existing vectors to identify similar matches. Semantic memory offers matches ranked by similarity rather than exact matches\n",
    "[Read More](https://learn.microsoft.com/en-us/semantic-kernel/memories/embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Memory\n",
    "In this example we are going to utilize Kernel Memory to perform all RAG tasks behind the scene. \n",
    "\n",
    "1. Extract text: recognize the file format and extract the information\n",
    "2. Partition the text in small chunks, to optimize search\n",
    "3. Extract embedding using an LLM embedding generator\n",
    "4. Save embedding into a vector index such as Azure AI Search, Qdrant or other DBs.\n",
    "\n",
    "[Read More](https://github.com/microsoft/kernel-memory?tab=readme-ov-file)\n",
    "\n",
    "Note, Kernel Memory is an independent project which was originated from Semantic Kernel then seperated out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.KernelMemory.core, 0.29.240219.2</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.KernelMemory.core,  0.29.240219.2\"\n",
    "#!import config/Settings.cs \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: Microsoft.KernelMemory.Handlers.TextExtractionHandler[0]\n",
      "      Handler 'extract' ready\n",
      "info: Microsoft.KernelMemory.Handlers.TextPartitioningHandler[0]\n",
      "      Handler 'partition' ready\n",
      "info: Microsoft.KernelMemory.Handlers.SummarizationHandler[0]\n",
      "      Handler 'summarize' ready\n",
      "info: Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler[0]\n",
      "      Handler 'gen_embeddings' ready, 1 embedding generators\n",
      "info: Microsoft.KernelMemory.Handlers.SaveRecordsHandler[0]\n",
      "      Handler save_records ready, 1 vector storages\n",
      "info: Microsoft.KernelMemory.Handlers.DeleteDocumentHandler[0]\n",
      "      Handler 'private_delete_document' ready\n",
      "info: Microsoft.KernelMemory.Handlers.DeleteIndexHandler[0]\n",
      "      Handler 'private_delete_index' ready\n",
      "info: Microsoft.KernelMemory.Handlers.DeleteGeneratedFilesHandler[0]\n",
      "      Handler 'delete_generated_files' ready\n"
     ]
    }
   ],
   "source": [
    "using Microsoft.KernelMemory;\n",
    "\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "    var embeddingConfig = new AzureOpenAIConfig\n",
    "    {\n",
    "        APIKey = apiKey,\n",
    "        Deployment = \"text-embedding-ada-002\",\n",
    "        Endpoint = azureEndpoint,\n",
    "        APIType = AzureOpenAIConfig.APITypes.EmbeddingGeneration,\n",
    "        Auth = AzureOpenAIConfig.AuthTypes.APIKey\n",
    "    };\n",
    "\n",
    "    var chatConfig = new AzureOpenAIConfig\n",
    "    {\n",
    "        APIKey = apiKey,\n",
    "        Deployment = model,\n",
    "        Endpoint = azureEndpoint,\n",
    "        APIType = AzureOpenAIConfig.APITypes.ChatCompletion,\n",
    "        Auth = AzureOpenAIConfig.AuthTypes.APIKey\n",
    "    };\n",
    "\n",
    "var memory = new KernelMemoryBuilder()\n",
    "    // .WithOpenAIDefaults(env[\"OPENAI_API_KEY\"])\n",
    "    .WithAzureOpenAITextGeneration(chatConfig)\n",
    "    .WithAzureOpenAITextEmbeddingGeneration(embeddingConfig)\n",
    "    .WithSimpleVectorDb()\n",
    "    .Build<MemoryServerless>();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the name of the Prince?\n",
      "\n",
      "Answer: The name of the Prince is the Happy Prince.\n"
     ]
    }
   ],
   "source": [
    "await memory.ImportDocumentAsync(\"./pdf/TheHappyPrince.pdf\", documentId: \"doc001\");\n",
    "\n",
    "var question = \"What is the name of the Prince?\";\n",
    "\n",
    "var answer = await memory.AskAsync(question);\n",
    "\n",
    "Console.WriteLine($\"Question: {question}\\n\\nAnswer: {answer.Result}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who the Prince is talking to in the story?\n",
      "\n",
      "Answer: In the story, the Prince is not directly conversing with anyone. The narrative describes the interactions between a Swallow and various elements of the story, such as the Reed and the statue of the Happy Prince. The Swallow, upon deciding to rest between the feet of the statue of the Happy Prince, is the character who experiences the events, such as feeling drops of water fall on him, which he initially mistakes for rain. The confusion arises from the fact that the Swallow is the one engaging with the environment and the statue, not the Prince himself engaging in a conversation.\n"
     ]
    }
   ],
   "source": [
    " question = \"Who the Prince is talking to in the story?\";\n",
    " answer = await memory.AskAsync(question);\n",
    "\n",
    "Console.WriteLine($\"Question: {question}\\n\\nAnswer: {answer.Result}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a simple RAG - use Semantic Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.SemanticKernel, 1.3.1</span></li><li><span>Microsoft.SemanticKernel.Plugins.Memory, 1.3.1-alpha</span></li><li><span>system.linq.async, 6.0.1</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.SemanticKernel, 1.3.1\"\n",
    "#r \"nuget: Microsoft.SemanticKernel.Plugins.Memory, 1.3.1-alpha\"\n",
    "#r \"nuget: System.Linq.Async, 6.0.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "source": [
    "Instead of KernelMemory, we are using Semantic Memory here. For the difference between KernelMemory and Semantic Memory, chere [here](https://microsoft.github.io/kernel-memory/#kernel-memory-km-and-semantic-memory-sm);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using Microsoft.SemanticKernel.Memory;\n",
    "\n",
    "#pragma warning disable SKEXP0003, SKEXP0011, SKEXP0052\n",
    "var memoryBuilder = new MemoryBuilder();\n",
    "\n",
    "var memory = memoryBuilder\n",
    "    .WithMemoryStore(new VolatileMemoryStore())\n",
    "    .WithAzureOpenAITextEmbeddingGeneration(\"text-embedding-ada-002\", azureEndpoint, apiKey, model)\n",
    "    .Build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#!import config/Settings.cs\n",
    "#!import plugins/PdfFilesPlugin\n",
    "\n",
    "using Microsoft.SemanticKernel;\n",
    "using Kernel = Microsoft.SemanticKernel.Kernel;\n",
    "using Microsoft.SemanticKernel.Connectors.OpenAI;\n",
    "var builder = Kernel.CreateBuilder();\n",
    "\n",
    "// Configure AI service credentials used by the kernel\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "builder.AddAzureOpenAIChatCompletion(model, azureEndpoint, apiKey);\n",
    "\n",
    "var kernel = builder.Build();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "csharp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
