{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab3-RAG-KernelMemory-Embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have seen a lots of applications like \"Chat to your own data\" or so on. This is the most common LLM architecture parent name RAG . A simple RAG flow will be like\n",
    "\n",
    "1 Retrieval:\n",
    "retrievs relevant information from a pre-existing database, knowledge graph, or corpus. This retrieved information serves as contextual knowledge for the generative model.\n",
    "The retrieval step involves querying the database using techniques like keyword search, semantic similarity, or more advanced methods like dense retrieval.\n",
    "\n",
    "2 Augmentation:\n",
    "\n",
    "The retrieved information is then integrated or augmented into the generative model.\n",
    "This augmentation process enhances the generative model's understanding of the context by providing relevant background information.\n",
    "\n",
    "3 Generation:\n",
    "\n",
    "With the augmented context, the generative model produces output.\n",
    "This output could be in the form of text generation, such as answering a question, completing a sentence, or generating a full document.\n",
    "\n",
    "4 Refinement (optional):\n",
    "Optionally, the generated output can undergo refinement or post-processing steps to ensure coherence, correctness, and fluency.\n",
    "Refinement techniques may include language model fine-tuning, paraphrasing, or other forms of text improvement.\n",
    "\n",
    "5 Output:\n",
    "\n",
    "The final output is delivered to the user or downstream application.\n",
    "This output benefits from both the generative capabilities of the model and the contextual knowledge retrieved during the process.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a simple RAG \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "The first step is to load our data into a vector store. \n",
    "\n",
    "Text information is saved as a long vector of numbers, known as \"embeddings.\" \n",
    "Meaning similarity of store text can be measured by the distance between 2 vectors in a high-dimensional space. \n",
    "When a query is made, it's converted into an embedding vector and compared to existing vectors to find similar matches. \n",
    "Semantic memory provides matches ranked by similarity rather than exact matches\n",
    "[Ref](https://learn.microsoft.com/en-us/semantic-kernel/memories/embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Memory\n",
    "We are going to utilize Kernel Memory to perform all RAG tasks behind the scene. \n",
    "\n",
    "1. Extract text: recognize the file format and extract the information\n",
    "2. Partition the text in small chunks, to optimize search\n",
    "3. Extract embedding using an LLM embedding generator\n",
    "4. Save embedding into a vector index such as Azure AI Search, Qdrant or other DBs.\n",
    "\n",
    "[Ref](https://github.com/microsoft/kernel-memory?tab=readme-ov-file)\n",
    "\n",
    "Note, Kernel Memory is an independent project which was originated from Semantic Kernel then seperated out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Microsoft.KernelMemory.core, 0.29.240219.2</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Microsoft.KernelMemory.core,  0.29.240219.2\"\n",
    "#!import config/Settings.cs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info: Microsoft.KernelMemory.Handlers.TextExtractionHandler[0]\n",
      "      Handler 'extract' ready\n",
      "info: Microsoft.KernelMemory.Handlers.TextPartitioningHandler[0]\n",
      "      Handler 'partition' ready\n",
      "info: Microsoft.KernelMemory.Handlers.SummarizationHandler[0]\n",
      "      Handler 'summarize' ready\n",
      "info: Microsoft.KernelMemory.Handlers.GenerateEmbeddingsHandler[0]\n",
      "      Handler 'gen_embeddings' ready, 1 embedding generators\n",
      "info: Microsoft.KernelMemory.Handlers.SaveRecordsHandler[0]\n",
      "      Handler save_records ready, 1 vector storages\n",
      "info: Microsoft.KernelMemory.Handlers.DeleteDocumentHandler[0]\n",
      "      Handler 'private_delete_document' ready\n",
      "info: Microsoft.KernelMemory.Handlers.DeleteIndexHandler[0]\n",
      "      Handler 'private_delete_index' ready\n",
      "info: Microsoft.KernelMemory.Handlers.DeleteGeneratedFilesHandler[0]\n",
      "      Handler 'delete_generated_files' ready\n"
     ]
    }
   ],
   "source": [
    "using Microsoft.KernelMemory;\n",
    "\n",
    "var (useAzureOpenAI, model, azureEndpoint, apiKey, orgId) = Settings.LoadFromFile();\n",
    "\n",
    "    var embeddingConfig = new AzureOpenAIConfig\n",
    "    {\n",
    "        APIKey = apiKey,\n",
    "        Deployment = \"text-embedding-ada-002\",\n",
    "        Endpoint = azureEndpoint,\n",
    "        APIType = AzureOpenAIConfig.APITypes.EmbeddingGeneration,\n",
    "        Auth = AzureOpenAIConfig.AuthTypes.APIKey\n",
    "    };\n",
    "\n",
    "    var chatConfig = new AzureOpenAIConfig\n",
    "    {\n",
    "        APIKey = apiKey,\n",
    "        Deployment = model,\n",
    "        Endpoint = azureEndpoint,\n",
    "        APIType = AzureOpenAIConfig.APITypes.ChatCompletion,\n",
    "        Auth = AzureOpenAIConfig.AuthTypes.APIKey\n",
    "    };\n",
    "\n",
    "var memory = new KernelMemoryBuilder()\n",
    "    // .WithOpenAIDefaults(env[\"OPENAI_API_KEY\"])\n",
    "    .WithAzureOpenAITextGeneration(chatConfig)\n",
    "    .WithAzureOpenAITextEmbeddingGeneration(embeddingConfig)\n",
    "    .WithSimpleVectorDb()\n",
    "    .Build<MemoryServerless>();\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "await memory.ImportDocumentAsync(\"sample-SK-Readme.pdf\", documentId: \"doc001\");\n",
    "\n",
    "var question = \"What's Semantic Kernel?\";\n",
    "\n",
    "var answer = await memory.AskAsync(question);\n",
    "\n",
    "Console.WriteLine($\"Question: {question}\\n\\nAnswer: {answer.Result}\");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "csharp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
